{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20806124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "# input_root = \"audio\"\n",
    "# output_root = \"audio_wav\"\n",
    "\n",
    "# for subdir in os.listdir(input_root):\n",
    "#     input_subdir = os.path.join(input_root, subdir)\n",
    "#     output_subdir = os.path.join(output_root, subdir)\n",
    "\n",
    "#     if not os.path.isdir(input_subdir):\n",
    "#         continue\n",
    "\n",
    "#     os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "#     for filename in os.listdir(input_subdir):\n",
    "#         if filename.endswith(\".mp3\"):\n",
    "#             input_path = os.path.join(input_subdir, filename)\n",
    "#             output_path = os.path.join(output_subdir, filename.replace(\".mp3\", \".wav\"))\n",
    "\n",
    "#             try:\n",
    "#                 audio = AudioSegment.from_mp3(input_path)\n",
    "#                 audio = audio.set_channels(1).set_frame_rate(32000)\n",
    "#                 audio.export(output_path, format=\"wav\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ö†Ô∏è Failed to convert: {input_path}\")\n",
    "#                 print(f\"   Reason: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af5016f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>no voice</th>\n",
       "      <th>singer</th>\n",
       "      <th>duet</th>\n",
       "      <th>plucking</th>\n",
       "      <th>hard rock</th>\n",
       "      <th>world</th>\n",
       "      <th>bongos</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>female singing</th>\n",
       "      <th>...</th>\n",
       "      <th>rap</th>\n",
       "      <th>metal</th>\n",
       "      <th>hip hop</th>\n",
       "      <th>quick</th>\n",
       "      <th>water</th>\n",
       "      <th>baroque</th>\n",
       "      <th>women</th>\n",
       "      <th>fiddle</th>\n",
       "      <th>english</th>\n",
       "      <th>mp3_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>f/american_bach_soloists-j_s__bach_solo_cantat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c/lvx_nova-lvx_nova-01-contimune-30-59.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c/lvx_nova-lvx_nova-01-contimune-175-204.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c/lvx_nova-lvx_nova-01-contimune-233-262.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c/lvx_nova-lvx_nova-01-contimune-291-320.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0/american_bach_soloists-j_s__bach__cantatas_v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clip_id  no voice  singer  duet  plucking  hard rock  world  bongos  \\\n",
       "0        2         0       0     0         0          0      0       0   \n",
       "1        6         0       0     0         0          0      0       0   \n",
       "2       10         0       0     0         0          0      0       0   \n",
       "3       11         0       0     0         0          0      0       0   \n",
       "4       12         0       0     0         0          0      0       0   \n",
       "5       14         0       0     0         0          0      0       0   \n",
       "6       19         0       0     0         0          0      0       0   \n",
       "7       21         0       0     0         0          0      0       0   \n",
       "8       23         0       0     0         0          0      0       0   \n",
       "9       25         0       0     0         0          0      0       0   \n",
       "\n",
       "   harpsichord  female singing  ...  rap  metal  hip hop  quick  water  \\\n",
       "0            0               0  ...    0      0        0      0      0   \n",
       "1            0               0  ...    0      0        0      0      0   \n",
       "2            0               0  ...    0      0        0      0      0   \n",
       "3            0               0  ...    0      0        0      0      0   \n",
       "4            0               0  ...    0      0        0      0      0   \n",
       "5            0               0  ...    0      0        0      0      0   \n",
       "6            0               0  ...    0      0        0      0      0   \n",
       "7            0               0  ...    0      0        0      0      0   \n",
       "8            0               0  ...    0      0        0      0      0   \n",
       "9            0               0  ...    0      0        0      0      0   \n",
       "\n",
       "   baroque  women  fiddle  english  \\\n",
       "0        0      0       0        0   \n",
       "1        1      0       0        0   \n",
       "2        0      0       0        0   \n",
       "3        0      0       0        0   \n",
       "4        0      0       0        0   \n",
       "5        0      0       0        0   \n",
       "6        0      0       0        0   \n",
       "7        0      0       0        0   \n",
       "8        0      0       0        0   \n",
       "9        0      0       0        0   \n",
       "\n",
       "                                            mp3_path  \n",
       "0  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "1  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "2  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "3  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "4  f/american_bach_soloists-j_s__bach_solo_cantat...  \n",
       "5         c/lvx_nova-lvx_nova-01-contimune-30-59.mp3  \n",
       "6       c/lvx_nova-lvx_nova-01-contimune-175-204.mp3  \n",
       "7       c/lvx_nova-lvx_nova-01-contimune-233-262.mp3  \n",
       "8       c/lvx_nova-lvx_nova-01-contimune-291-320.mp3  \n",
       "9  0/american_bach_soloists-j_s__bach__cantatas_v...  \n",
       "\n",
       "[10 rows x 190 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "csv_path = \"./annotations_final.csv\"\n",
    "df = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ba0745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 25863 rows\n"
     ]
    }
   ],
   "source": [
    "# size of the dataset\n",
    "print(f\"Dataset size: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131ef8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./audio_wav/magnatagatune_text_audio_pairs.jsonl'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Get the tag columns (all except clip_id and mp3_path)\n",
    "tag_columns = df.columns.difference(['clip_id', 'mp3_path'])\n",
    "\n",
    "# Output list\n",
    "samples = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tags = [tag.replace(\"_\", \" \") for tag in tag_columns if row[tag] == 1]\n",
    "    if not tags:\n",
    "        continue  # skip samples with no tags\n",
    "    \n",
    "    # Generate simple natural language prompt\n",
    "    prompt = f\"A music clip with \" + \", \".join(tags[:-1]) + (\" and \" + tags[-1] if len(tags) > 1 else tags[0]) + \".\"\n",
    "\n",
    "    samples.append({\n",
    "        \"audio_filepath\": f\"audio_wav/{row['mp3_path'].replace('.mp3', '.wav')}\",\n",
    "        \"text\": prompt\n",
    "    })\n",
    "\n",
    "# Save to JSONL\n",
    "jsonl_path = \"./audio_wav/magnatagatune_text_audio_pairs.jsonl\"\n",
    "with open(jsonl_path, \"w\") as f:\n",
    "    for sample in samples:\n",
    "        f.write(json.dumps(sample) + \"\\n\")\n",
    "\n",
    "jsonl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374af278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'audio_wav/magnatagatune_tokenized_32khz.jsonl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EncodecModel, AutoProcessor\n",
    "import librosa\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_32khz\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_32khz\")\n",
    "\n",
    "input_jsonl = \"audio_wav/magnatagatune_text_audio_pairs.jsonl\"\n",
    "output_jsonl = \"audio_wav/magnatagatune_tokenized_32khz.jsonl\"\n",
    "Path(input_jsonl).parent.mkdir(parents=True, exist_ok=True)\n",
    "Path(output_jsonl).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_SAMPLES = 100\n",
    "\n",
    "with open(input_jsonl, \"r\") as fin, open(output_jsonl, \"w\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "        item = json.loads(line)\n",
    "        audio_path = item[\"audio_filepath\"]\n",
    "\n",
    "        try:\n",
    "            waveform, sr = librosa.load(audio_path, sr=None, mono=True)\n",
    "            # print(f\"Processing {audio_path} with shape {waveform.shape} and sample rate {sr}\")\n",
    "            inputs = processor(raw_audio=waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                codes = outputs.audio_codes[0].cpu().tolist()\n",
    "\n",
    "            item[\"audio_tokens\"] = codes\n",
    "            fout.write(json.dumps(item) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed on {audio_path}: {e}\")\n",
    "\n",
    "output_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2bc3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_wav/flattened_token_with_text_embedding.jsonl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import json\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text_encoder = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "text_encoder.eval()\n",
    "\n",
    "input_path = \"audio_wav/magnatagatune_tokenized_32khz.jsonl\"\n",
    "output_path = \"audio_wav/flattened_token_with_text_embedding.jsonl\"\n",
    "\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_SAMPLES = 100\n",
    "\n",
    "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        if idx >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "        item = json.loads(line)\n",
    "        text = item[\"text\"]\n",
    "        tokens = item[\"audio_tokens\"] \n",
    "\n",
    "        try:\n",
    "            transposed = list(zip(*tokens)) \n",
    "            flattened = [tok for frame in transposed for tok in frame] \n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "                embedding = text_encoder(**encoded).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "\n",
    "            fout.write(json.dumps({\n",
    "                \"text\": text,\n",
    "                \"text_embedding\": embedding,\n",
    "                \"audio_flat_tokens\": flattened\n",
    "            }) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on sample {idx}: {e}\")\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb161aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'audio_wav/musicgen_dataset.pt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List\n",
    "\n",
    "# ÂÆö‰πâ Dataset Á±ª\n",
    "class MusicGenDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, max_audio_tokens: int = 1024):\n",
    "        self.data = []\n",
    "        with open(jsonl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                audio_tokens = item[\"audio_flat_tokens\"][:max_audio_tokens]\n",
    "                text_embed = item[\"text_embedding\"]\n",
    "\n",
    "                embed_dim = 768\n",
    "                num_text_tokens = 8\n",
    "                if len(text_embed) != embed_dim:\n",
    "                    continue\n",
    "\n",
    "                text_tokens = torch.tensor(text_embed, dtype=torch.float).repeat(num_text_tokens, 1)\n",
    "                text_token_ids = torch.full((num_text_tokens,), -100)\n",
    "\n",
    "                MAX_AUDIO_TOKENS = 2048\n",
    "                flat_audio = [tok for frame in zip(*audio_tokens) for tok in frame]\n",
    "                flat_audio = flat_audio[:MAX_AUDIO_TOKENS + 1]\n",
    "                input_ids = torch.tensor(flat_audio[:-1], dtype=torch.long)\n",
    "                labels    = torch.tensor(flat_audio[1:], dtype=torch.long) \n",
    "\n",
    "                self.data.append({\n",
    "                    \"text_embed\": text_tokens,         # shape: [N_text, 768]\n",
    "                    \"input_ids\": input_ids,            # shape: [T]\n",
    "                    \"labels\": labels,                  # shape: [T]\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# ÊûÑÂª∫ Dataset Âπ∂‰øùÂ≠òÁî®‰∫éËÆ≠ÁªÉ\n",
    "dataset_path = \"audio_wav/flattened_token_with_text_embedding.jsonl\"\n",
    "dataset = MusicGenDataset(dataset_path)\n",
    "\n",
    "# ‰øùÂ≠ò‰∏∫ .pt Êñá‰ª∂\n",
    "torch.save(dataset, \"audio_wav/musicgen_dataset.pt\")\n",
    "\"audio_wav/musicgen_dataset.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MusicGenTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, n_layers=6, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.prefix_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=n_heads, dim_feedforward=2048, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, text_embed, input_ids):\n",
    "        tok_emb = self.token_embedding(input_ids)   # [B, T, 768]\n",
    "        if tok_emb.dim() == 4:\n",
    "            tok_emb = tok_emb.squeeze(2)\n",
    "\n",
    "        prefix = self.prefix_proj(text_embed)       # [B, 8, 768]\n",
    "        x = torch.cat([prefix, tok_emb], dim=1)     # [B, 8+T, 768]\n",
    "\n",
    "        attn_mask = torch.triu(torch.ones(x.size(1), x.size(1)), 1).bool().to(x.device)\n",
    "        out = self.transformer(x, mask=attn_mask)\n",
    "        return self.lm_head(out[:, prefix.size(1):])  # return only audio token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "605a5941",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m MusicGenTransformer(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, embed_dim\u001b[38;5;241m=\u001b[39membed_dim)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     23\u001b[0m scaler \u001b[38;5;241m=\u001b[39m GradScaler()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # for debugging\n",
    "jsonl_path = \"audio_wav/flattened_token_with_text_embedding.jsonl\"\n",
    "vocab_size = 1024\n",
    "embed_dim = 768\n",
    "max_audio_tokens = 1024\n",
    "batch_size = 1\n",
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "max_len = 1024 \n",
    "# dataset = torch.load(\"audio_wav/musicgen_dataset.pt\", weights_only=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = MusicGenDataset(jsonl_path, max_audio_tokens=max_audio_tokens)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MusicGenTransformer(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        text_embed = batch[\"text_embed\"].to(device)              # [B, 8, 768]\n",
    "        input_ids  = batch[\"input_ids\"].to(device)[:, :max_len]  # [B, T]\n",
    "        labels     = batch[\"labels\"].to(device)[:, :max_len]      # [B, T]\n",
    "\n",
    "        # üîí Ê£ÄÊü•Ê†áÁ≠æÂêàÊ≥ï\n",
    "        if labels.max() >= vocab_size:\n",
    "            print(\"‚ö†Ô∏è Ë∑≥Ëøá batch: label Ë∂äÁïå\")\n",
    "            continue\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(text_embed, input_ids)                # [B, T, V]\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=-100  # ÂèØÈÄâ\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8093ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "text_embed = torch.randn(1, 8, 768).to(device)\n",
    "\n",
    "\n",
    "input_ids = torch.full((1, 1), fill_value=0, dtype=torch.long).to(device)  # ÂàùÂßã token\n",
    "\n",
    "generated = []\n",
    "max_len = 1000\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_len):\n",
    "        logits = model(text_embed, input_ids)  # [B, T, V]\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # greedy\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        generated.append(next_token.item())\n",
    "\n",
    "audio_tokens = input_ids[0, 1:].cpu().tolist()\n",
    "\n",
    "\n",
    "codec = EncodecModel.from_pretrained(\"facebook/encodec_32khz\").to(\"cuda\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_32khz\")\n",
    "\n",
    "num_codebooks = 8\n",
    "audio_tokens_tensor = torch.tensor(audio_tokens).view(1, -1)  # [1, T]\n",
    "audio_tokens_tensor = audio_tokens_tensor.expand(num_codebooks, -1).unsqueeze(0)  # [1, 8, T]\n",
    "\n",
    "with torch.no_grad():\n",
    "    wav = codec.decode(audio_tokens_tensor)[0]  # [1, 1, samples]\n",
    "    wav = wav.squeeze().cpu()  # [samples]\n",
    "\n",
    "torchaudio.save(\"output.wav\", wav.unsqueeze(0), 32000)\n",
    "print(\"result saved as output.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
